---
title: Some Notes of the LLaMA Paper
tags: [Notes, Paper Reading, LLMs]
style: fill
color: primary
description: Some notes of the LLaMA paper.
---

# [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)

- Approach
  - Pre-training Data
    - English CommonCrawl [67%]
    - C4 [15%]
    - Gutenberg and Books3 [4.5%]
    - ArXiv [2.5%]
    - Stack Exchange [2%]
  - Tokenizer
    - byte-pair encoding (BPE) algorithm
      - [explaination](https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)
  - Architecture
    - Pre-normalization [GPT3]
      - normalize the input of each transformer sub-layer
        - to improve the training stability
      - [RMSNorm](https://arxiv.org/abs/1910.07467)
    - [SwiGLU](https://arxiv.org/pdf/2002.05202v1.pdf) activation function [PaLM]
      - [SWISH: A SELF-GATED ACTIVATION FUNCTION](https://arxiv.org/pdf/1710.05941v1.pdf?source=post_page)
    - Rotary Embeddings [GPTNeo]
      - add [rotary positional embeddings (RoPE)](https://arxiv.org/pdf/2104.09864v5.pdf)
  - Optimizer
    - AdamW
  - Efficient implementation
    - an efficient implementation of the causal multi-head attention
      - to reduce memory usage and runtime
        - by not storing the attention weights and not computing the key/query scores that are masked
      - available in the xformers library
    - reduced the amount of activations that are recomputed during the backward pass with checkpointing
      - by manually implementing the backward function for the transformer layers, instead of relying on the PyTorch autograd.
    - using model and sequence parallelism
- Main results
  - Common Sense Reasoning
    - BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA
  - Closed-book Question Answering
    - Natural Questions, TriviaQA
  - Reading Comprehension
    - RACE reading comprehension benchmark
  - Mathematical reasoning
    - MATH, GSM8k
  - Code generation
    - HumanEval, MBPP
  - Massive Multitask Language Understanding
    - MMLU
- [Instruction Finetuning](https://arxiv.org/pdf/2210.11416.pdf)
