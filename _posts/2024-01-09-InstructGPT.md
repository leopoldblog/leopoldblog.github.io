---
title: Some Notes of the InstructGPT Paper and RLHF
tags: [Notes, Paper Reading, LLMs]
style: fill
color: primary
description: Some notes of the InstructGPT paper and RLHF. (not complete)
---
# [Training language models to follow instructionswith human feedback](https://arxiv.org/abs/2203.02155)

## Introduction

- use reinforcement learning from human feedback to fine-tune GPT-3 to follow a broad class of written instructions
- InstructGPT models show improvements in truthfulness over GPT-3
- InstructGPT shows small improvements in toxicity over GPT-3, but not bias
- InstructGPT models show promising generalization to instructions outside of the RLHF finetuning distribution

## Methods and experimental details

### High-level methodology

- Collect demonstration data, and train a supervised policy
- Collect comparison data, and train a reward model
- Optimize a policy against the reward model using PPO

### Models

- Supervised fine-tuning (SFT)
  - We fine-tune GPT-3 on our labeler demonstrations using supervised learning
- Reward modeling (RM)
  - Starting from the SFT model with the final unembedding layer removed,
    we trained a model to take in a prompt and response, and output a scalar reward
- Reinforcement learning (RL)
  - fine-tuned the SFT model on our environment using PPO

### Evaluation

- the model should follow instructions, but also infer intention from a few-shot prompt or another interpretable pattern
- Evaluations on API distribution
  - human preference ratings on a held out set of prompts from the same source as our training distribution
- Evaluations on public NLP datasets

## Discussion

### Implications for alignment research

- The cost of increasing model alignment is modest relative to pretraining.
- We’ve seen some evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in
- We were able to mitigate most of the performance degradations introduced by our fine-tuning.
- We’ve validated alignment techniques from research in the real world

### Open questions

- Many methods could be tried to further decrease the models’ propensity to generate toxic, biased, or otherwise harmful outputs.
- Getting models to do what we want is directly related to the steerability and controllability literature
- there are many other algorithms that could be used to train policies on our demonstration and comparison data to get even better results


# [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

## RLHF: Let’s take it step by step

### Pretraining language models

- In general, there is not a clear answer on “which model” is the best for the starting point of RLHF
  - the design space of options in RLHF training are not thoroughly explored

### Reward model training

- The underlying goal
  - get a model or system
    - takes in a sequence of text
    - returns a scalar reward
      - which should numerically represent the human preference
- The training dataset of prompt-generation pairs
  - generated by sampling a set of prompts from a predefined dataset
    - OpenAI used prompts submitted by users to the GPT API
- Human annotators are used to rank the generated text outputs from the LM
  - rankings create a much better regularized dataset

### Fine-tuning with RL

## Open-source tools for RLHF

## What’s next for RLHF?
